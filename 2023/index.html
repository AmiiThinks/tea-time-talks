<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="">
  <meta name="author" content="">

  <title>2023 Tea Time Talks Abstracts</title>

  <!-- Bootstrap core CSS -->
  <link href="dist/css/bootstrap.min.css" rel="stylesheet">
</head>

<body>

  <!-- Begin page content -->
  <div class="jumbotron">
    <div class="container">
      <div class="page-header">
        <h1>2023 Tea Time Talks</h1>
      </div>
    </div>
  </div>

  <div class="container">
    <font size="+1" face = "sans serif">
      <p>
        <b>Banafsheh Rafiee (June 8, 2023; CSC 3-33)</b><br>
        <i>Auxiliary task discovery through generate and test</i>
        <p align="justify">
          In this talk, I will present an approach to auxiliary task discovery in reinforcement learning based on ideas from representation learning. Auxiliary tasks tend to improve data efficiency by forcing the agent to learn auxiliary prediction and control objectives in addition to the main task of maximizing reward, and thus producing better representations. Typically these tasks are designed by people. Meta-learning offers a promising avenue for automatic task discovery; however, these methods are computationally expensive and challenging to tune in practice. In this talk, I will present a complementary approach to the auxiliary task discovery: continually generating new auxiliary tasks and preserving only those with high utility. I also introduce a new measure of auxiliary tasks' usefulness based on how useful the features induced by them are for the main task. The proposed algorithm significantly outperforms random tasks and learning without auxiliary tasks across a suite of environments. 
        </p>
      </p>
      <br>

      <!-- <p>
        <b>Matt Taylor (June 14, 2023; Amii)</b><br>
        <i>Humans: Who needs them?</i><br>
        <p align="justify">
        Reinforcement learning is amazing.
        But where does the MDP actually come from? I’m going to argue in this that humans are critical to the RL lifecycle. Moreover, I’ll argue that significant improvements to existing explainability techniques are required to fully invest humans, whether lay people, subject matter experts, or machine learning experts, with the ability required to drive this human-agent interaction for Human-In-The-Loop RL.<br>
        </p>
      </p>
      <br> -->

      <p>
        <b>Shibhansh Dohare (June 20, 2023; CSC 3-33)</b><br>
        <i>Towards Overcoming Policy Collapse in Deep Reinforcement Learning</i>
        <p align="justify">
          A long-awaited characteristic of reinforcement learning agents is scalable performance, that is, to continue to learn and improve performance with a never-ending stream of experience. However, current deep reinforcement learning algorithms are known to be brittle and difficult to train, which limits their scalability. For example, the learned policy can dramatically worsen after some initial training as the agent continues to interact with the environment. We call this phenomenon \textit{policy collapse}. We first establish that policy collapse can occur in both policy gradient and value-based methods. Policy collapse happens in these algorithms in typical benchmarks such as Mujoco environments when trained with their commonly used hyper-parameters. In a simple 2-state MDP, we show that the standard use of the Adam optimizer with its default hyper-parameters is a root cause of policy collapse. Specifically, the standard use of Adam can lead to sudden large weight changes even when the gradient is small whenever there is non-stationarity in the data stream. We find that policy collapse can be largely mitigated by using the same hyper-parameters for the running averages of the first and second moments of the gradient. Additionally, we find that aggressive L2 regularization also mitigates policy collapse in many cases. Our work establishes that a minimal change in the existing usage of deep reinforcement learning can reduce policy collapse and enable more stable and scalable deep reinforcement learning.
        </p>
      </p>
      <br>

      <p>
        <b>Montaser Mohammedalamen (June 21, 2023; Amii)</b><br>
        <i>Learning To Be Cautious</i>
        <p align="justify">
          A key challenge in the field of reinforcement learning is to develop agents that behave cautiously in novel situations. It is generally impossible to anticipate all situations that an autonomous system may face or what behavior would best avoid bad outcomes. An agent that could learn to be cautious would overcome this challenge by discovering for itself when and how to behave cautiously. In contrast, current approaches typically embed task-specific safety information or explicit cautious behaviors into the system, which is error-prone and imposes extra burdens on practitioners. In this paper, we present both a sequence of tasks where cautious behavior becomes increasingly non-obvious, as well as an algorithm to demonstrate that it is possible for a system to \emph{learn} to be cautious. The essential features of our algorithm are that it characterizes reward function uncertainty without task-specific safety information and uses this uncertainty to construct a robust policy. Specifically, we construct robust policies with a k-of-N counterfactual regret minimization (CFR) subroutine given a learned reward function uncertainty represented by a neural network ensemble belief. These policies exhibit caution in each of our tasks without any task-specific safety tuning.
        </p>
      </p>
      <br>

      <p>
        <b>Sumedh Pendurkar (June 22, 2023; CSC 3-33)</b><br>
        <i>Bilevel Entropy based Mechanism Design for Balancing Meta in Video Games</i>
        <p align="justify">
          In this talk, I will present a mechanism design problem where the goal of the designer is to maximize the entropy of a player’s mixed strategy at a Nash equilibrium. This objective is of special relevance to video games where game designers wish to diversify the players’ interaction with the game. To solve this mechanism design problem, I will present a bi-level alternating optimization technique that (1) approximates the mixed strategy Nash equilibrium using a Nash Monte-Carlo reinforcement learning approach and (2) applies a gradient-free optimization technique (Covariance-Matrix Adaptation Evolutionary Strategy) to maximize the entropy of the mixed strategy obtained in level (1). The experimental results show that the proposed approach achieves comparable results to the state-of-the-art approach on three benchmark domains “Rock-Paper-Scissors-Fire-Water”, “Workshop Warfare” and “Pokemon Video Game Championship”. Next, I will present our empirical findings that, unlike previous state-of-the-art approaches, the computational complexity of our proposed approach scales significantly better in larger combinatorial strategy spaces.
        </p>
      </p>
      <br>

      <p>
        <b>Alex Lewandowski (June 27, 2023; CSC 3-33)</b><br>
        <i>Small Continual Learning Problems: Time Helps Prevent Plasticity Loss</i>
        <p align="justify">
          I will argue that time plays an important, but mysterious, role in continual learning. The talk will begin by discussing fundamental issues in continual learning, focusing on plasticity loss in neural networks. I will then present preliminary work on a simple one-dimensional regression problem that leads to plasticity loss, even for a deep and wide neural network. The main finding is that plasticity loss can be prevented by allowing the neural network to observe a representation of time - even though the representation of time is uncorrelated with the regression problem. I will conclude with next steps for understanding the role of time in continual learning.
        </p>
      </p>
      <br>

      <p>
        <b>Khurram Javed (June 28, 2023; Amii)</b><br>
        <i>Demystifying Why Larger Neural Networks Generalize Better</i>
        <p align="justify">
          A large part of the ML community believes that larger neural networks generalize better. In this talk, I will share some experiments that show that larger models indeed generalize better in supervised learning tasks. I will, then, try to demystify the reason behind their better generalization. Finally, I will demonstrate how we can potentially exploit our newfound understanding to achieve better generalization with smaller models.
        </p>
      </p>
      <br>

      <p>
        <b>Levi Lelis (June 29, 2023; CSC 3-33)</b><br>
        <i>Programmatic Policies: Are They Worth the Trouble?</i>
        <p align="justify">
        In this presentation, we will explore the concept of using computer programs to represent policies, commonly referred to as programmatic policies in the literature. Programmatic policies offer several advantages, including better generalization to unseen environments and interpretability. However, the main challenge associated with programmatic policies is the need to search through extensive and often discontinuous program spaces to synthesize them. So, here's the big question: Are they worth the hassle?
        </p>
      </p>
      <br>

      <p>
        <b>Revan MacQueen (July 4, 2023; CSC 3-33)</b><br>
        <i>Guarantees for Self-Play In Zero-Sum Games</i>
        <p align="justify">
        Guarantees for Self-Play In Zero-Sum Games and Beyond Game theory offers strong theoretical guarantees for self-play in two-player zero-sum games. I will describe my work generalizing these guarantees to multi-player general-sum games.
        </p>
      </p>
      <br>

      <p>
        <b>Kris De Asis (July 5, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Arsalan Sharifnassab (July 11, 2023; CSC 3-33)</b><br>
        <i>Toward Efficient Gradient-Based Value Estimation in Reinforcement Learning</i>
      </p>
      <br>

      <p>
        <b>Prabhat Nagarajan (July 12, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Marlos C. Machado (July 18, 2023; CSC 3-33)</b><br>
        <i>Deep Laplacian-based Options for Temporally-Extended Exploration</i><br>
        <p align="justify">
          Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL).
          An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options.
          A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian.
          Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated,
          (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase.
          These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up options-based exploration.
          To do so, we introduce a fully online deep RL algorithm for discovering Laplacian-based options and evaluate our approach on a variety of pixel-based tasks.
          We compare to several state-of-the-art exploration methods and show that our approach is effective, general, and especially promising in non-stationary settings.
        </p>
      </p>
      <br>

      <p>
        <b>Michael Ogezi (July 19, 2023; Amii)</b><br>
        <i>RL-based Discrete Optimization of Negative Prompts for Image Generation</i><br>
        <p align="justify">
        In text-to-image generation, negative prompts have been employed to instruct models on what not to do. Building upon this idea, we propose a method for discrete optimization of these negative prompts through supervised fine-tuning and reinforcement learning, with the aim of enhancing the generated images. Utilizing this combined approach, our method yields significant improvements. Empirical evidence includes a 40% increase in the Inception Score and a surpassing of ground-truth negative prompts within the training dataset. In addition to our method, we present the first publicly accessible dataset designed explicitly for research involving negative prompts.
        </p>
      </p>
      <br>

      <p>
        <b>Subhojeet Pramanik (July 20, 2023; CSC 3-33)</b><br>
        <!-- <i>The Tunnel Effect in Neural Network Representations</i><br> -->
        <!-- <p align="justify"> -->
        <!-- </p> -->
      </p>
      <br>

      <p>
        <b>Ehsan Imani (July 25, 2023; CSC 3-33)</b><br>
        <i>The Tunnel Effect in Neural Network Representations</i><br>
        <!-- <p align="justify"> -->
        <!-- </p> -->
      </p>
      <br>

      <p>
        <b>Edan Meyer (July 26, 2023; Amii)</b><br>
        <i>Investigating Discrete World Models</i><br>
        <p align="justify">
          Many recent approaches in model-based RL opt to learn world models over discrete latent representations of states.
          Let's take a deeper look into the motivation for this decision, and see if it provides any benefits.
        </p>
      </p>
      <br>

      <p>
        <b>Michael Przystupa (July 27, 2023; CSC 3-33)</b><br>
        <i>Properties of Latent Action Models for Robotics</i><br>
        <p align="justify">
          Learning optimal policies in large action spaces can be computationally expensive.
          Therefore, one research direction has proposed that learning an intermediate model that maps latent actions to the original action space can improve learning sample efficiency.
          Empirical research results support this claim that these compressed action spaces have benefits in reinforcement learning and robotic teleoperation.
          However, there needs to be more work providing any guarantees of these representations in reachability in an agent's state space.
          In this talk, we propose that, at least for robotic applications, one can derive theoretical navigation guarantees in relevant portions of the state space that can impact navigation in the entire state space.
        </p>
      </p>
      <br>

      <p>
        <b>Martha Steenstrup (August 1, 2023; CSC 3-33)</b><br>
        <i>Reinforcement learning applied to control problems in communications</i><br>
      </p>
      <br>

      <p>
        <b>Aidan Bush (August 2, 2023; Amii)</b><br>
        <!-- <i>Reinforcement learning applied to control problems in communications</i><br> -->
      </p>
      <br>

      <p>
        <b>Martha White (August 3, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Samuel Robertson (August 8, 2023; CSC 3-33)</b><br>
        <i>Computational and Statistical Bounds for Linear Bandits</i><br>
      </p>
      <br>

      <p>
        <b>Mohamed Elsayed (August 9, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Fengdi Che (August 10, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Homayoon Farrahi (August 15, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Abhishek Naik (August 16, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Yu WANG (August 17, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Shivam Garg (August 22, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Gautham Vasan (August 23, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Rich Sutton (August 24, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <br>
      <br>
      <p><i>The 2023 tea time talks are coordinated by Yanqing Wu (yanqing.wu AT ualberta DOT ca).</i></p>
    </div>

    <footer class="footer">
      <div class="container">
        <p class="text-muted"><br></p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery.min.js"><\/script>')</script>
    <script src="js/bootstrap.min.js"></script>
  </body>
  </html>
