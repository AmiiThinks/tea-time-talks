<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
  <meta name="description" content="">
  <meta name="author" content="">

  <title>2023 Tea Time Talks Abstracts</title>

  <!-- Bootstrap core CSS -->
  <link href="dist/css/bootstrap.min.css" rel="stylesheet">
</head>

<body>

  <!-- Begin page content -->
  <div class="jumbotron">
    <div class="container">
      <div class="page-header">
        <h1>2023 Tea Time Talks</h1>
      </div>
    </div>
  </div>

  <div class="container">
    <font size="+1" face = "sans serif">
      <p>
        <b>Banafsheh Rafiee (June 8, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Matt Taylor (June 14, 2023; Amii)</b><br>
        <i>Humans: Who needs them?</i><br>
        <p align="justify">
        Reinforcement learning is amazing.
        But where does the MDP actually come from? I’m going to argue in this that humans are critical to the RL lifecycle. Moreover, I’ll argue that significant improvements to existing explainability techniques are required to fully invest humans, whether lay people, subject matter experts, or machine learning experts, with the ability required to drive this human-agent interaction for Human-In-The-Loop RL.<br>
        </p>
      </p>
      <br>

      <p>
        <b>Shibhansh Dohare (June 20, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Montaser Mohammedalamen (June 21, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Sumedh Pendurkar (June 22, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Alex Lewandowski (June 27, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Khurram Javed (June 28, 2023; Amii)</b><br>
        <i>Resource constrained real-time continual learning</i>
      </p>
      <br>

      <p>
        <b>Revan MacQueen (July 4, 2023; CSC 3-33)</b><br>
        <i>Guarantees for Self-Play In Zero-Sum Games</i>
        <p align="justify">
        Guarantees for Self-Play In Zero-Sum Games and Beyond Game theory offers strong theoretical guarantees for self-play in two-player zero-sum games. I will describe my work generalizing these guarantees to multi-player general-sum games.
        </p>
      </p>
      <br>

      <p>
        <b>Kris De Asis (July 5, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Prabhat Nagarajan (July 12, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Michael Ogezi (July 19, 2023; Amii)</b><br>
        <i>RL-based Discrete Optimization of Negative Prompts for Image Generation</i><br>
        <p align="justify">
        In text-to-image generation, negative prompts have been employed to instruct models on what not to do. Building upon this idea, we propose a method for discrete optimization of these negative prompts through supervised fine-tuning and reinforcement learning, with the aim of enhancing the generated images. Utilizing this combined approach, our method yields significant improvements. Empirical evidence includes a 40% increase in the Inception Score and a surpassing of ground-truth negative prompts within the training dataset. In addition to our method, we present the first publicly accessible dataset designed explicitly for research involving negative prompts.
        </p>
      </p>
      <br>

      <p>
        <b>Marlos C. Machado (July 18, 2023; CSC 3-33)</b><br>
        <i>Deep Laplacian-based Options for Temporally-Extended Exploration</i><br>
        <p align="justify">
          Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL).
          An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options.
          A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian.
          Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated,
          (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase.
          These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up options-based exploration.
          To do so, we introduce a fully online deep RL algorithm for discovering Laplacian-based options and evaluate our approach on a variety of pixel-based tasks.
          We compare to several state-of-the-art exploration methods and show that our approach is effective, general, and especially promising in non-stationary settings.
        </p>
      </p>
      <br>

      <p>
        <b>Michael Przystupa (July 27, 2023; CSC 3-33)</b><br>
        <i>Properties of Latent Action Models for Robotics</i><br>
        <p align="justify">
          Learning optimal policies in large action spaces can be computationally expensive.
          Therefore, one research direction has proposed that learning an intermediate model that maps latent actions to the original action space can improve learning sample efficiency.
          Empirical research results support this claim that these compressed action spaces have benefits in reinforcement learning and robotic teleoperation.
          However, there needs to be more work providing any guarantees of these representations in reachability in an agent's state space.
          In this talk, we propose that, at least for robotic applications, one can derive theoretical navigation guarantees in relevant portions of the state space that can impact navigation in the entire state space.
        </p>
      </p>
      <br>

      <p>
        <b>Martha Steenstrup (August 1, 2023; CSC 3-33)</b><br>
        <i>Reinforcement learning applied to control problems in communications</i><br>
      </p>
      <br>

      <p>
        <b>Martha White (August 3, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Samuel Robertson (August 8, 2023; CSC 3-33)</b><br>
        <i>Computational and Statistical Bounds for Linear Bandits</i><br>
      </p>
      <br>

      <p>
        <b>Mohamed Elsayed (August 9, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Fengdi Che (August 10, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Homayoon Farrahi (August 15, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Abhishek Naik (August 16, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Yu WANG (August 17, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Shivam Garg (August 22, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <p>
        <b>Gautham Vasan (August 23, 2023; Amii)</b><br>
      </p>
      <br>

      <p>
        <b>Rich Sutton (August 24, 2023; CSC 3-33)</b><br>
      </p>
      <br>

      <br>
      <br>
      <p><i>The 2023 tea time talks are coordinated by Yanqing Wu (yanqing.wu AT ualberta DOT ca).</i></p>
    </div>

    <footer class="footer">
      <div class="container">
        <p class="text-muted"><br></p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery.min.js"><\/script>')</script>
    <script src="js/bootstrap.min.js"></script>
  </body>
  </html>
