<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>2019 Tea Time Talks Abstracts & PDFs</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
  </head>

  <body>

  <!-- Begin page content -->
  <div class="jumbotron">
      <div class="container">
          <div class="page-header">
              <h1>2019 Tea Time Talk Abstracts & PDFs</h1>
          </div>

          <p class="lead">
            Some presenters have generously offered to share the slides from their talk.  If available, you will be able to click on a presenter's name to download a PDF of the slides used during their talk.
          </p>
      </div><!-- /.container -->
  </div>

    <div class="container">
      <font size="+1" face = "sans serif">
      <p><b>Rich Sutton (May 27, 2019)</b><br>
        Topic: Open Questions in Model-based Reinforcement Learning<br>
        Abstract: Reinforcement learning methods that learn a model of the world from their action-observation data, and then plan with that model, may be the next big thing in artificial intelligence. Let us assume a Dyna-like architecture, in which everything---learning, planning, and acting---is done online and continually. There appear to be natural extensions of the original Dyna architecture to stochastic dynamics, function approximation, partial observability, temporal abstraction, and average reward, but there are complications when these extensions are done together, leading to many open questions. Should the model generate samples, or expectations? Should the value function be linear in the state features? If the value function is linear, then should the model be also? How exactly should planning be done with average reward? Should an option model incorporate the average reward estimate? How should planning and state update interrelate? This talk will provide an introduction to these questions.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Shangtong_Zhang.pdf">Shangtong Zhang (May 29, 2019)</a></b><br>
        Topic: Generalized Off-Policy Actor-Critic<br>
        Abstract: We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL) setting. Compared to the commonly used excursion objective, which can be misleading about the performance of the target policy when deployed, our new objective better predicts such performance. We prove the Generalized Off-Policy Policy Gradient Theorem to compute the policy gradient of the counterfactual objective and use an emphatic approach to get an unbiased sample from this policy gradient, yielding the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm. We demonstrate the merits of Geoff-PAC over existing algorithms in Mujoco robot simulation tasks, the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.</p>
      <br>
      <p><b>Martha Steenstrup (May 30, 2019)</b><br>
        Topic: Reinforcement Learning for Control of Communications Networks<br>
        Abstract: A communications network is a dynamic distributed system whose behavior depends on the properties of its component devices, the characteristics of the environment in which it operates, the behavior of its users, and the set of control algorithms that determine where, when, and how data is transported among communicating devices.  Most network controllers have been engineered based on assumptions about the behavior of users and the operating environment as well as the effects of interactions among controllers.  Moreover, network controllers act in response to information about network state that may be delayed, partial, and noisy and often do so autonomously and asynchronously.  Our hypothesis is that network controllers that can autonomously learn effective control policies without innate knowledge of detailed behavioral models or supervisory input, make appropriate decisions under uncertainty, and select useful features from observations of network, user, and environment state will be superior to human-engineered solutions in terms of accuracy, agility, robustness, and efficiency.  We are in the early stages of testing this hypothesis on a particular network control problem: congestion control.  In this talk, we introduce the congestion control problem, identify the challenges to developing effective reinforcement-learning-based solutions, and argue that communications networks are an ideal domain in which to probe the limits of
        reinforcement learning.</p>
      <br>
      <p><b>Harm van Seijen (June 3, 2019)</b></p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Kris_De_Asis.pdf">Kris De Asis (June 5, 2019)</a></b><br>
        Topic: Finite-horizon Temporal Difference Methods<br>
        Abstract: Reinforcement learning problems are often concerned with an arbitrarily long discounted sum of rewards, denoted as the return. In episodic settings, it is the sum of rewards from from a given point until the end of the episode, and in continuing settings, it is an infinite sum of rewards. The discount rate in the return is typically used to specify a horizon of interest, as it decays the weight given to distant rewards in the sum. We refer to the sum up until the very end (or infinity) as the infinite-horizon return. An appeal of the infinite-horizon return is that it can be expressed recursively, giving rise to algorithms which try to find the fixed-point of self-satisfying relationships. Another appeal is that the complexity of these algorithms does not vary with how far an agent tries to predict. In this talk, we look at estimating finite-horizon returns, where the sum is only considered up to a fixed number of steps into the future. We explore the space of finite-horizon temporal difference methods, including one which scales sub-linearly with horizon. We note properties of the resulting algorithms, and argue some benefits over the infinite-horizon setting in both prediction and control.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Yi_Wan.pdf">Yi Wan (June 6, 2019)</a></b><br>
        Topic: Planning with Expectation Models<br>
        Abstract: This talk discusses the expectation model for model-based reinforcement learning. I will formalize its definition, compare it with other model choices, discuss its different parameterization choices, and propose a new Gradient-based Dyna-style planning algorithm, which has convergence guarantee even if the approximate expectation model is flawed.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Andy_Patterson.pdf">Andy Patterson (June 10, 2019)</a></b><br>
        Topic: Importance Sampling Ratio Placement for Gradient-TD Methods<br>
        Abstract: Using the importance sampling ratio to correct samples in off-policy learning can lead to high variance updates and unstable learning. We can reduce the variance in off-policy learning by altering the placement of the importance sampling ratio correction term within the update. In this talk, I will discuss the importance sampling ratio correction placement within the updates for Gradient-TD algorithms. I will show analytically that using the importance sampling ratio to correct the entire TD error term can be viewed as a control variate form of the alternative placement strategy, using the importance sampling ratio to correct only the temporal difference target. Finally, I will empirically support these insights on a simple domain, demonstrating increased stability and faster learning for the control variate form of the importance sampling ratio placement strategy.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Adam_Parker.pdf">Adam Parker (June 12, 2019)</a></b><br>
        Topic: Human-Machine Learning Interactions<br>
        Abstract: Humans are increasingly interacting with not only machines, but machine-learning systems. Thus, there is an increasing need to think about how those interactions will take place and how interactions can be facilitated smoothly and effectively. In rehabilitation science in particular, strong emphasis is placed on human-human and human-technology interaction. Clinicians of all sorts regularly address the needs of their human patients using the “person first” mentality. Using this same person-first viewpoint that is growing in rehabilitation science along with prosthetic limbs as an example rehabilitation robot, we can think about machine-learning problems in new ways. This talk will therefore discuss the reasons, to and benefits of, viewing communication with another agent as actions. At least initially this does not require modifications of current machine-learning methods, but changes in how we think about problems. Rather than learning about the world and how to interact with it, the system can be thought of as learning about the user and how to interact with them. Machine learning agents that determine how to succeed at a task by communicating with each other without specific knowledge of language has been shown in previous studies on emergent communication. There is also a body of research on human-human interaction, termed joint action, which highlights the importance of understanding the other agent in a shared task, as well as non-verbal communication. These ideas are of value to the field of machine-learning and can be used to develop the interactions between humans and machine-learning agents.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Varun_Bhatt.pdf">Varun Bhatt (June 13, 2019)</a></b><br>
        Topic: Training Multiple Intelligent Agents to Communicate<br>
        Abstract: Communication is one of the key aspects of human survival. Humans rely extensively on communication to both learn quickly and to act efficiently in environments featuring cooperation. As artificial intelligence becomes commonplace in the real world, intelligent agents need to be able to communicate with other learning agents and with humans. In this talk, I will be motivating the need for agents to communicate and talking about the current methods and some of the challenges. In particular, I will be focusing on the issue of credit assignment, since it is not obvious whether the reward was a result of poor signal by the sender or poor actions taken by the receiver.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Khurram_Javed.pdf">Khurram Javed (June 17, 2019)</a></b><br>
        Topic: Meta-Learning Representations for Continual Learning<br>
        Abstract: A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on artificial neural network function approximators arguably do the opposite— they are highly prone to forgetting and rarely trained to facilitate future learning. The primary reason for this poor behavior is the catastrophic interference problem plaguing neural networks.<br>
        In this talk, I will first give a structured overview of the interference problem in neural networks and identify three factors that, when combined together, result in catastrophic interference. I will then present my work done in collaboration with Martha White on learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. The core idea is to use gradient based meta-learning to treat forgetting as a training signal for learning a representation. I will show that it is possible to learn representations that are more effective for online updating and that sparsity naturally emerges in these representations. Finally, I will demonstrate that a basic online updating strategy with our learned representation is competitive with existing experience replay based methods for continual learning.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Patrick_Pilarski.pdf">Patrick Pilarski (June 19, 2019)</a></b><br>
        Topic: The Role of Prediction in Joint Action</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Andrew_Jacobsen.pdf">Andrew Jacobsen (June 20, 2019)</a></b><br>
        Topic: A Value Function Basis for Multi-step Prediction<br>
        Abstract: The ability to make accurate predictions about future observations constitutes a fundamental form of awareness and understanding of one's surroundings. An autonomous agent which can make accurate predictions about various aspects of its own sensory-motor stream possesses a valuable tool for reasoning about its environment. In an ideal world, agents could learn to predict all aspects of their sensory-motor stream, but in practice there are limitations on the number of things that can be learned in real time.  In this talk, we will discuss how an agent can leverage a small set of learned General Value Function (GVF) predictions to accurately infer the answers to a wide variety of questions about its sensory-motor stream. I will show experimental results in which we accurately infer hundreds of GVF predictions and multi-step predictions about the sensor readings of a mobile robot, using only a small set of learned GVFs. Finally, I will talk about a simple case in which all expected future observations can be completely characterized in terms of a basis of GVF predictions.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Martha_White.pdf">Martha White (June 24, 2019)</a></b><br>
        Topic: Some Thoughts About Learning Predictions Online<br>
        Abstract: In this talk, I will discuss some of what my group has realized about learning neural networks and representations online, as well as some of my running hypotheses. The goal of the talk is to (a) make concrete statements about conventional wisdom, which is not always written down; (b) clarify some misconceptions and (c) facilitate a discussion on this topic that is relevant to many in ML and AI at the UofA.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Matthew_Schlegel.pdf">Matthew Schlegel (June 26, 2019)</a></b><br>
        Topic: Importance Resampling<br>
        Abstract:  Importance sampling (IS) is a common reweighting strategy for off-policy prediction in reinforcement learning. While importance sampling is consistent and unbiased, it can result in high variance updates to the weights for the value function. One can consider a resampling strategy as an alternative to reweighting, avoiding the use of importance sampling ratios in the update. This talk will introduce importance resampling and provide high level conclusions of its use in off-policy prediction through empirical results. Further details can be found in the arxiv paper <a href="https://arxiv.org/abs/1906.04328">https://arxiv.org/abs/1906.04328</a>, currently in submission to NeurIPS.</p>
      <br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Hado_van_Hasselt.pdf">Hado van Hasselt (July 4, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Paniz_Behboudian.pdf">Paniz Behboudian (July 15, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Csaba_Szepesvari.pdf">Csaba Szepesvari (July 17, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Parash_Rahman.pdf">Parash Rahman (July 18, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Joseph_Jay_Williams.pdf">Joseph Jay Williams (July 22, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Alex_Lewandowski.pdf">Alex Lewandowski (August 7, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Banafsheh_Rafiee.pdf">Banafsheh Rafiee (August 12, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Shibhansh_Dohare.pdf">Shibhansh Dohare (August 14, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Eric_Graves.pdf">Eric Graves (August 19, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Raksha_Kumaraswamy.pdf">Raksha Kumaraswamy (August 21, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Yash_Satsangi.pdf">Yash Satsangi (August 26, 2019)</a><br>
    </div>

    <footer class="footer">
      <div class="container">
        <p class="text-muted"><br></p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery.min.js"><\/script>')</script>
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>
