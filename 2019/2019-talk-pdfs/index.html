<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>2019 Tea Time Talks Abstracts & PDFs</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
  </head>

  <body>

  <!-- Begin page content -->
  <div class="jumbotron">
      <div class="container">
          <div class="page-header">
              <h1>2019 Tea Time Talk Abstracts & PDFs</h1>
          </div>

          <p class="lead">
            Some presenters have generously offered to share the slides from their talk.  If available, click on a presenter's name to download the slides used during their talk.
          </p>
      </div><!-- /.container -->
  </div>

    <div class="container">
      <font size="+1" face = "sans serif">
      <p><b>Rich Sutton (May 27, 2019)</b><br>
        Topic: Open Questions in Model-based Reinforcement Learning<br>
        Abstract: Reinforcement learning methods that learn a model of the world from their action-observation data, and then plan with that model, may be the next big thing in artificial intelligence. Let us assume a Dyna-like architecture, in which everything---learning, planning, and acting---is done online and continually. There appear to be natural extensions of the original Dyna architecture to stochastic dynamics, function approximation, partial observability, temporal abstraction, and average reward, but there are complications when these extensions are done together, leading to many open questions. Should the model generate samples, or expectations? Should the value function be linear in the state features? If the value function is linear, then should the model be also? How exactly should planning be done with average reward? Should an option model incorporate the average reward estimate? How should planning and state update interrelate? This talk will provide an introduction to these questions.</p>
      <br>
      <p><b><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Shangtong_Zhang.pdf">Shangtong Zhang (May 29, 2019)</a></b><br>
        Topic: Generalized Off-Policy Actor-Critic<br>
        Abstract: We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL) setting. Compared to the commonly used excursion objective, which can be misleading about the performance of the target policy when deployed, our new objective better predicts such performance. We prove the Generalized Off-Policy Policy Gradient Theorem to compute the policy gradient of the counterfactual objective and use an emphatic approach to get an unbiased sample from this policy gradient, yielding the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm. We demonstrate the merits of Geoff-PAC over existing algorithms in Mujoco robot simulation tasks, the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.</p>

      <p>Martha Steenstrup (May 30, 2019)<br>
        Topic: Reinforcement Learning for Control of Communications Networks<br>
        Abstract: A communications network is a dynamic distributed system whose behavior depends on the properties of its component devices, the characteristics of the environment in which it operates, the behavior of its users, and the set of control algorithms that determine where, when, and how data is transported among communicating devices.  Most network controllers have been engineered based on assumptions about the behavior of users and the operating environment as well as the effects of interactions among controllers.  Moreover, network controllers act in response to information about network state that may be delayed, partial, and noisy and often do so autonomously and asynchronously.  Our hypothesis is that network controllers that can autonomously learn effective control policies without innate knowledge of detailed behavioral models or supervisory input, make appropriate decisions under uncertainty, and select useful features from observations of network, user, and environment state will be superior to human-engineered solutions in terms of accuracy, agility, robustness, and efficiency.  We are in the early stages of testing this hypothesis on a particular network control problem: congestion control.  In this talk, we introduce the congestion control problem, identify the challenges to developing effective reinforcement-learning-based solutions, and argue that communications networks are an ideal domain in which to probe the limits of
        reinforcement learning.</p>

      <p>Harm van Seijen (June 3, 2019)</p>

      <p><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Kris_De_Asis.pdf">Kris De Asis (June 5, 2019)</a><br>
        Topic: Finite-horizon Temporal Difference Methods<br>
        Abstract: Reinforcement learning problems are often concerned with an arbitrarily long discounted sum of rewards, denoted as the return. In episodic settings, it is the sum of rewards from from a given point until the end of the episode, and in continuing settings, it is an infinite sum of rewards. The discount rate in the return is typically used to specify a horizon of interest, as it decays the weight given to distant rewards in the sum. We refer to the sum up until the very end (or infinity) as the infinite-horizon return. An appeal of the infinite-horizon return is that it can be expressed recursively, giving rise to algorithms which try to find the fixed-point of self-satisfying relationships. Another appeal is that the complexity of these algorithms does not vary with how far an agent tries to predict. In this talk, we look at estimating finite-horizon returns, where the sum is only considered up to a fixed number of steps into the future. We explore the space of finite-horizon temporal difference methods, including one which scales sub-linearly with horizon. We note properties of the resulting algorithms, and argue some benefits over the infinite-horizon setting in both prediction and control.</p>

      <p><a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Yi_Wan.pdf">Yi Wan (June 6, 2019)</a><br>
        Topic: Planning with Expectation Models<br>
        Abstract: This talk discusses the expectation model for model-based reinforcement learning. I will formalize its definition, compare it with other model choices, discuss its different parameterization choices, and propose a new Gradient-based Dyna-style planning algorithm, which has convergence guarantee even if the approximate expectation model is flawed.</p>

      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Andy_Patterson.pdf">Andy Patterson (June 10, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Adam_Parker.pdf">Adam Parker (June 12, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Varun_Bhatt.pdf">Varun Bhatt (June 13, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Khurram_Javed.pdf">Khurram Javed (June 17, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Patrick_Pilarski.pdf">Patrick Pilarski (June 19, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Andrew_Jacobsen.pdf">Andrew Jacobsen (June 20, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Martha_White.pdf">Martha White (June 24, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Matthew_Schlegel.pdf">Matthew Schlegel (June 26, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Hado_van_Hasselt.pdf">Hado van Hasselt (July 4, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Paniz_Behboudian.pdf">Paniz Behboudian (July 15, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Csaba_Szepesvari.pdf">Csaba Szepesvari (July 17, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Parash_Rahman.pdf">Parash Rahman (July 18, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Joseph_Jay_Williams.pdf">Joseph Jay Williams (July 22, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Alex_Lewandowski.pdf">Alex Lewandowski (August 7, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Banafsheh_Rafiee.pdf">Banafsheh Rafiee (August 12, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Shibhansh_Dohare.pdf">Shibhansh Dohare (August 14, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Eric_Graves.pdf">Eric Graves (August 19, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Raksha_Kumaraswamy.pdf">Raksha Kumaraswamy (August 21, 2019)</a><br>
      <a href="https://amiithinks.github.io/tea-time-talks/2019-talk-pdfs/Yash_Satsangi.pdf">Yash Satsangi (August 26, 2019)</a><br>
    </div>

    <footer class="footer">
      <div class="container">
        <p class="text-muted"><br></p>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="js/vendor/jquery.min.js"><\/script>')</script>
    <script src="js/bootstrap.min.js"></script>
  </body>
</html>
